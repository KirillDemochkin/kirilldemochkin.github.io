---
layout: post
title: "üèÜ: Casual GAN Papers Awards"
meta_title: "Casual GAN Papers Awards"
description: "The First Annual Casual GAN Papers Awards for the Year 2021"
date: 2022-1-28 16:00:00 -0000
categories: the-best-machine-learning-ai-papers-2021-ranked
---

#### The First Annual Casual GAN Papers Awards for the Year 2021

##### ‚≠êRanking difficulty: üåïüåïüåïüåïüåï

***

![Casual GAN Papers Awards](/assets/images/awards_2021.png "Casual GAN Papers Awards")

##### üéØ At a glance:

Hi everyone!  

There is an ‚ÄúX‚Äù of the year award in pretty much every industry ever, and ranking things is fun, which is reason enough for us to hold the first annual Casual GAN Papers Awards for the year 2021!  

***

##### üöÄ Motivation:

This isn‚Äôt going to be a simple top-5 list, since pretty much all of the papers I covered this year are the cream of the crop in what they do, as judged by yours truly and my imaginary council of distinguished ML experts! The purpose of this post is simply to celebrate the amazing achievements in machine learning research over the last year and highlight some of the larger trends that I have noticed while analyzing the papers I read every week.

***

##### üèÜ Awards!:

Without further ado I would like to present:  

‚≠êÔ∏è The ‚ÄúStop Ignoring it Already‚Äù award to [Guided Diffusion](https://www.casualganpapers.com/guided_diffusion_langevin_dynamics_classifier_guidance/Guided-Diffusion-explained.html). I totally ignored this paper for the better part of the year cause it always looked intimidating and too math-heavy for my taste. Nevertheless, the intuition behind the main ideas turned out to be surprisingly insightful and easy to grasp. Moreover, as I mentioned in the paper digest, I believe diffusion to be one of the most important research topics to track in 2022.  

‚≠êÔ∏è The ‚Äú#Trending‚Äù award goes to [Dream Fields](https://www.casualganpapers.com/image-editing-stylegan2-encoder-generator-tuning-inversion/DreamFields-explained.html). This paper combines the two biggest trends of 2021 in generative modeling (3D-aware Image synthesis and CLIP guidance) glued together by several clever tricks to obtain some of the most mind-bending gifs with results that I have seen this year.  

‚≠êÔ∏è The ‚ÄúWill it CLIP?‚Äù award to [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://www.casualganpapers.com/clip-guided-text-based-semantic-image-editing/StyleCLIP-explained.html). I spent several hours playing with the demo for StyleCLIP when it first came out, and it still amazes me just how well CLIP works out of the box. I also believe that StyleCLIP is responsible for sparking the interest for several other CLIP-based editing papers released after.  

‚≠êÔ∏è The ‚Äú20 Pages and counting‚Äù award to [Alias-free GAN](https://www.casualganpapers.com/alias-free-stylegan-2-without-texture-sticking/AliasFreeGAN.html). This was one of the longest papers that I read last year, and boy, writing the post about StyleGAN3 took me forever. I am, nevertheless, glad I did it, even if the successor never quite took off like the original.  

‚≠êÔ∏è The ‚ÄúMost Transformative‚Äù award to [Compositional Transformers for Scene Generation](https://www.casualganpapers.com/gan-transformer-object-based-layout-generation/GANsformer2-explained.html). GANsformer2 doesn‚Äôt even use a transformer, and instead makes the most logical and interesting (in my opinion) use of the self-attention mechanism (Duplex Attention) in generative networks to date. Not only is the image generated using self-attention, but the entire layout of the scene is also fully controllable via the same model.  

‚≠êÔ∏è The ‚ÄúPaper name of the year‚Äù award goes to [ü¶ô LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions](https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html). I just love this name, simple, memorable, logical, and it has an emoji in the title!  

‚≠êÔ∏è The ‚ÄúFastest Man Alive‚Äù award goes to [Projected GANs](https://www.casualganpapers.com/data-efficient-fast-gan-training-small-datasets/ProjectedGAN-explained.html) & [Plenoxels](https://www.casualganpapers.com/nerf-3d-voxels-without-neural-networks/Plenoxels-explained.html). AI is an arms race, and what better way to get ahead than by making things faster? Both papers introduce incredible speedups to processes that used to take forever, and can now be done orders of magnitude quicker.  

‚≠êÔ∏è The ‚ÄúYou can tell the authors are having fun‚Äù award goes to [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](https://www.casualganpapers.com/text-guided-clip-gan-domain-adaptation/StyleGAN-NADA-explained.html) for the most hilarious samples in the paper. ‚ÄúDog -> Nicolas Cage‚Äù?! Are you kidding me, this is almost too good to be true.  

‚≠êÔ∏è The ‚ÄúThis came out of nowhere‚Äù award: Let‚Äôs be honest, [CLIP](https://www.casualganpapers.com/zero-shot-contrastive-loss-image-text-pretraining/CLIP-explained.html) is hyped, perhaps too much for its own good, and while I can not say for sure that we will see as many CLIP-based papers in 2022, we got to pay respects to this model since it was almost stealth-released in the shadow of the much more heavily promoted DALL-E, which itself somehow never saw the light of day. Despite the lack of initial promotion, CLIP quickly took over the generative art community, giving birth to many amazing creators, who continue to find new ways to improve on the initial concept and evolve the generation techniques to create true pieces of art with CLIP. Also a quick shoutout to the Moscow-based SberAI team for making not one but several models the center point of attention of the generative art community with their RuDALL-E, and Emojich models being the highlights!  

##### üìà Experiment insights / Key takeaways:

***

##### üõ† Possible Improvements:

And that is it for the first annual Casual GAN Papers Awards for the year 2021!  

If I were to summarize the last year‚Äôs progress in a TLDR, I would say 2021 was all about using transformers in computer vision with obscene amounts of data, using CLIP to guide things, using knowledge of the 3D world to generate realistic 2D images, using the latent space of pretrained generators to edit images in all sort of fun ways, and of course, going on huggingface spaces to try out all the cool new demos in just a couple of clicks (not sponsored, but if Huggingface wants to sponsor me, I am all for it).  

##### ‚úèÔ∏èMy Notes:

- (5/5) A Great Year of Papers.  
- I might do my predictions for 2022 later, but that is an entire post of its own.  
- Anyways, there were so many good papers last year, I am sure I missed a few, so please, share your favorite papers in the comments!  

##### üî• Check Out These Popular AI Paper Summaries:
- [Train a NeRF in Seconds - Instant Neural Graphics explained]({% post_url 2022-1-18-Instant-Neural-Graphics-Primitives-explained %})
- [Infinite Video Generation - StyleGAN-V explained]({% post_url 2022-1-11-StyleGAN-V-explained %})
- [Guided Diffusion explained]({% post_url 2021-12-26-Guided-Diffusion-explained %})

##### üëã Thanks for reading!
<a href="https://www.patreon.com/bePatron?u=53448948" data-patreon-widget-type="become-patron-button">Join Patreon for Exclusive Perks!</a><script async src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>

*If you found this paper digest useful, **subscribe** and **share** the post with your friends and colleagues to support Casual GAN Papers!*

*[Join the Casual GAN Papers telegram channel](https://t.me/joinchat/KeutnzlvetRkZGZi) to stay up to date with new AI Papers!*

*[Discuss](https://t.me/casual_gans_chat) the paper*

***

By: [@casual_gan](https://t.me/joinchat/KeutnzlvetRkZGZi)

P.S. Send me paper suggestions for future posts
[@KirillDemochkin](mailto:kdemochkin@gmail.com)!
